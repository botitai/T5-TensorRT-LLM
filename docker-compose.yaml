services:
  tensorrt-llm:
    build:
      context: TensorRT-LLM
      target: release
      args:
        - BASE_IMAGE=nvcr.io/nvidia/tritonserver
        - BASE_TAG=23.10-pyt-python-py3
      dockerfile: docker/Dockerfile.multi
    image: tensorrt_llm:v0.6.1

  download:
    image: tensorrt_llm:trt-llm-backend
    working_dir: /app/tensorrt_llm/examples/flan_t5
    volumes:
      - type: bind
        source: TensorRT-LLM/examples/flan_t5/
        target: /app/tensorrt_llm/examples/flan_t5/
    env_file:
      - .env
    entrypoint: ["python3", "download.py"]

  build:
    image: tensorrt_llm:trt-llm-backend
    working_dir: /app/tensorrt_llm/examples/flan_t5
    runtime: nvidia
    volumes:
      - type: bind
        source: TensorRT-LLM/examples/flan_t5/
        target: /app/tensorrt_llm/examples/flan_t5/
      - type: bind
        source: TensorRT-LLM/tensorrt_llm/models/
        target: /usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/
    env_file:
      - .env
    entrypoint:
      [
        "python3",
        "build.py",
        "--model_dir=./models/",
        "--use_bert_attention_plugin",
        "--use_gpt_attention_plugin",
        "--dtype=float16",
        "--max_beam_width=1",
        "--max_input_len=1024",
        "--output_dir=${ENGINE_DIR}",
        "--dtype=float32"
      ]

  triton-server:
    image: tensorrt_llm:triton-backend
    runtime: nvidia
    environment:
      - PYTHONPATH=/models/flan_t5/1
    ports:
      - 8000:8000
      - 8001:8001
      - 8002:8002
    env_file:
      - .env
    volumes:
      - type: bind
        source: cache
        target: ${TRANSFORMERS_CACHE}
      - type: bind
        source: TensorRT-LLM/examples/flan_t5/${ENGINE_DIR}
        target: /trt_engines
      - type: bind
        source: models
        target: /models
    entrypoint:
      ["tritonserver", "--model-repository=/models", "--log-verbose=1"]

  trt-llm-environment:
    image: us-central1-docker.pkg.dev/got-it-ai/got-it-ai/tensorrt_llm:v0.6.1
    runtime: nvidia
    volumes:
      - type: bind
        source: TensorRT-LLM/examples/enc_dec
        target: /workdir
      - type: bind
        source: TensorRT-LLM/tensorrt_llm/models/
        target: /usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/
    entrypoint: ["bash", "-c", "while true; do sleep 1; done"]
  
